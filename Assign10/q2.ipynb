{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import Counter\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fi = open('./Brown_train.txt')\n",
    "\n",
    "sent_db = []\n",
    "tag_db = []\n",
    "\n",
    "\n",
    "\n",
    "for line in fi:\n",
    "    line = line.rstrip()\n",
    "    temp_line = line.split(' ')\n",
    "    temp_sent_db = []\n",
    "    temp_tag_db = []\n",
    "    \n",
    "    for word in temp_line:\n",
    "        temp_word = word.split('/')    \n",
    "        temp_sent_db.append(temp_word[0])\n",
    "        temp_tag_db.append(temp_word[1])\n",
    "    \n",
    "    sent_db.append(temp_sent_db)\n",
    "    tag_db.append(temp_tag_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27491 27491\n",
      "15 15\n"
     ]
    }
   ],
   "source": [
    "print(len(sent_db), len(tag_db))\n",
    "print(len(sent_db[0]), len(tag_db[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeVocab(data):\n",
    "    vocab = []\n",
    "    for d in data:\n",
    "        for w in d:\n",
    "            vocab.append(w)\n",
    "    \n",
    "    return sorted(set(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIndexInVocab(vocab, word):\n",
    "    if word not in vocab:\n",
    "        return 0\n",
    "    return vocab.index(word) + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(sentence, padLen):\n",
    "    pad_sent = []\n",
    "    i = 0\n",
    "    for s in sentence:\n",
    "        pad_sent.append(s)\n",
    "        i += 1\n",
    "        if i == padLen:\n",
    "            break\n",
    "    \n",
    "    rem_len = padLen - len(pad_sent)\n",
    "    for i in range(rem_len):\n",
    "        pad_sent.append(1)\n",
    "    \n",
    "    return pad_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeVectorized(list_sent, vocab, maxlen):\n",
    "    ans = []\n",
    "\n",
    "    for l in list_sent:\n",
    "        sent = []\n",
    "        for w in l:\n",
    "            sent.append(getIndexInVocab(vocab, w))\n",
    "        \n",
    "        sent = padding(sent, maxlen)\n",
    "        ans.append(sent)\n",
    "    \n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(tags, categories):\n",
    "    cat_sequences = []\n",
    "    for s in tags:\n",
    "        cats = []\n",
    "        for item in s:\n",
    "            cats.append(np.zeros(categories))\n",
    "            cats[-1][item] = 1.0\n",
    "        cat_sequences.append(cats)\n",
    "    return np.array(cat_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ignore_class_accuracy(to_ignore=0):\n",
    "    def ignore_accuracy(y_true, y_pred):\n",
    "        y_true_class = K.argmax(y_true, axis=-1)\n",
    "        y_pred_class = K.argmax(y_pred, axis=-1)\n",
    " \n",
    "        ignore_mask = K.cast(K.not_equal(y_pred_class, to_ignore), 'int32')\n",
    "        matches = K.cast(K.equal(y_true_class, y_pred_class), 'int32') * ignore_mask\n",
    "        accuracy = K.sum(matches) / K.maximum(K.sum(ignore_mask), 1)\n",
    "        return accuracy\n",
    "    return ignore_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(sent_db, tag_db, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_vocab = makeVocab(X_train)\n",
    "tag_vocab = makeVocab(y_train)\n",
    "x_train_vectorised = makeVectorized(X_train, sent_vocab, MAX_LENGTH)\n",
    "x_test_vectorised = makeVectorized(X_test,sent_vocab, MAX_LENGTH)\n",
    "\n",
    "y_train_vectorised = makeVectorized(y_train, tag_vocab, MAX_LENGTH)\n",
    "y_test_vectorised = makeVectorized(y_test,tag_vocab, MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_vectorised = np.array(x_train_vectorised)\n",
    "x_test_vectorised = np.array(x_test_vectorised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21992, 60, 39)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_ohe = to_categorical(y_train_vectorised, len(tag_vocab) + 2)\n",
    "y_test_ohe = to_categorical(y_test_vectorised, len(tag_vocab) + 2)\n",
    "y_train_ohe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 60, 128)           3873408   \n",
      "_________________________________________________________________\n",
      "bidirectional_10 (Bidirectio (None, 60, 512)           788480    \n",
      "_________________________________________________________________\n",
      "time_distributed_10 (TimeDis (None, 60, 39)            20007     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 60, 39)            0         \n",
      "=================================================================\n",
      "Total params: 4,681,895\n",
      "Trainable params: 4,681,895\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(MAX_LENGTH, )))\n",
    "model.add(Embedding(len(sent_vocab) + 2, 128))\n",
    "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
    "model.add(TimeDistributed(Dense(len(tag_vocab) + 2)))\n",
    "model.add(Activation('softmax'))\n",
    " \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(0.001),\n",
    "              metrics=['accuracy', ignore_class_accuracy(0), 'categorical_accuracy'])\n",
    " \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manishkumar/anaconda3/envs/btp_env/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17593 samples, validate on 4399 samples\n",
      "Epoch 1/5\n",
      "17593/17593 [==============================] - 217s 12ms/step - loss: 0.8718 - accuracy: 0.7441 - ignore_accuracy: 0.7444 - categorical_accuracy: 0.7441 - val_loss: 0.5571 - val_accuracy: 0.8437 - val_ignore_accuracy: 0.8432 - val_categorical_accuracy: 0.8437\n",
      "Epoch 2/5\n",
      "17593/17593 [==============================] - 174s 10ms/step - loss: 0.2767 - accuracy: 0.9232 - ignore_accuracy: 0.9234 - categorical_accuracy: 0.9232 - val_loss: 0.1034 - val_accuracy: 0.9711 - val_ignore_accuracy: 0.9710 - val_categorical_accuracy: 0.9711\n",
      "Epoch 3/5\n",
      "17593/17593 [==============================] - 176s 10ms/step - loss: 0.0590 - accuracy: 0.9845 - ignore_accuracy: 0.9846 - categorical_accuracy: 0.9845 - val_loss: 0.0479 - val_accuracy: 0.9862 - val_ignore_accuracy: 0.9862 - val_categorical_accuracy: 0.9862\n",
      "Epoch 4/5\n",
      "17593/17593 [==============================] - 170s 10ms/step - loss: 0.0283 - accuracy: 0.9923 - ignore_accuracy: 0.9923 - categorical_accuracy: 0.9923 - val_loss: 0.0399 - val_accuracy: 0.9878 - val_ignore_accuracy: 0.9877 - val_categorical_accuracy: 0.9878\n",
      "Epoch 5/5\n",
      "17593/17593 [==============================] - 186s 11ms/step - loss: 0.0201 - accuracy: 0.9943 - ignore_accuracy: 0.9943 - categorical_accuracy: 0.9943 - val_loss: 0.0379 - val_accuracy: 0.9882 - val_ignore_accuracy: 0.9881 - val_categorical_accuracy: 0.9882\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1a6543ab50>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train_vectorised, y_train_ohe, batch_size=128, epochs=5, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5499/5499 [==============================] - 24s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(x_test_vectorised,y_test_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.040078302439228365,\n",
       " 0.9876765608787537,\n",
       " 0.9876783490180969,\n",
       " 0.9876765608787537]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
