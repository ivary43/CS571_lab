{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = fetch_openml('mnist_784', version=1, return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "onehotencoder = OneHotEncoder(sparse=False)\n",
    "y_ohc = onehotencoder.fit_transform(y.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train , X_test , y_train, y_test = train_test_split(X, y_ohc, test_size = 0.20, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu (x):\n",
    "    return np.maximum(x, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_derivative (x):\n",
    "    x[x<=0] = 0\n",
    "    x[x>0] = 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid (x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative (x):\n",
    "    return x*(1-x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y, y_exp):\n",
    "    y_exp1 = y_exp.argmax(axis=1)\n",
    "    m = y_exp1.shape[0]\n",
    "    log_likelihood = -np.log(y[range(m), y_exp1])\n",
    "    loss = np.sum(log_likelihood)/m\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "#     z = np.sum(np.exp(x))\n",
    "#     return np.exp(x)/z\n",
    "#     y = np.amax(x, axis=1).T\n",
    "#     #z = np.multiply(np.ones((len(y),len(y))),y).T\n",
    "#     z = np.repeat(y, 10).reshape(56000,10)\n",
    "#     ex = np.exp(x-z)\n",
    "#     print('fsefsef')\n",
    "#     print(ex[0])\n",
    "#     return ex/np.sum(ex, axis=1, keepdims=True)\n",
    "    exps = np.exp(x - np.max(x))\n",
    "    return exps/np.sum(exps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_derivative(x):\n",
    "    y = x.reshape((-1,1))\n",
    "    jac = np.diagflat(x)-np.dot(x,x.T)\n",
    "    return jac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs = 1, learning_rate = 0.1):\n",
    "    inputs = X_train\n",
    "    inputs = inputs/255\n",
    "    exp_output = y_train\n",
    "    \n",
    "    input_layer_neurons = 784\n",
    "    hidden_layer_neurons = 256\n",
    "    output_layer_neurons = 10\n",
    "    \n",
    "    hidden_layer_weights = np.random.rand(input_layer_neurons, hidden_layer_neurons)\n",
    "    hidden_bias = np.random.rand(1,hidden_layer_neurons)\n",
    "    output_layer_weight = np.random.rand(hidden_layer_neurons,output_layer_neurons)\n",
    "    output_bias = np.random.rand(1,output_layer_neurons)\n",
    "    \n",
    "    loss = []\n",
    "    epochs_arr = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(inputs[0])\n",
    "        print(hidden_layer_weights[0])\n",
    "        print(inputs.shape, hidden_layer_weights.shape)\n",
    "        hidden_layer_activation = np.dot(inputs, hidden_layer_weights)\n",
    "        hidden_layer_activation += hidden_bias\n",
    "        print(hidden_layer_activation[0])\n",
    "        hidden_layer_output = relu(hidden_layer_activation)\n",
    "        \n",
    "        print(hidden_layer_output[0])\n",
    "        \n",
    "        output_layer_activation = np.dot(hidden_layer_output, output_layer_weight)\n",
    "        output_layer_activation += output_bias\n",
    "        \n",
    "        print(output_layer_activation)\n",
    "        \n",
    "        predicted_output = softmax(output_layer_activation)\n",
    "        #print(predicted_output[0])\n",
    "        \n",
    "        print(exp_output[:10])\n",
    "        print(predicted_output[:10])\n",
    "        \n",
    "        #error \n",
    "        error = cross_entropy_loss(predicted_output, exp_output)\n",
    "        derivative_predicted_output = error*softmax_derivative(predicted_output)\n",
    "        \n",
    "        error_hidden_layer = derivative_predicted_output.dot(output_layer_weight.T)\n",
    "        derivative_hidden_layer_output = error_hidden_layer*relu_derivative(hidden_layer_output)\n",
    "        \n",
    "        output_layer_weight += hidden_layer_output.T.dot(derivative_predicted_output) * learning_rate\n",
    "        output_bias += np.sum(derivative_predicted_output, axis=0 , keepdims=True) * learning_rate\n",
    "        hidden_layer_weights += inputs.T.dot(derivative_hidden_layer_output) *learning_rate\n",
    "        hidden_bias += np.sum(derivative_hidden_layer_output, axis=0, keepdims=True) *learning_rate\n",
    "        \n",
    "        epochs_arr.append(epoch+1)\n",
    "        loss.append(np.sum(error))\n",
    "        \n",
    "        print('epoch '+str(epoch))\n",
    "     \n",
    "    return epochs_arr, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.3372549\n",
      " 0.99607843 0.79215686 0.02352941 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.16470588 0.70980392 0.99215686 0.51764706\n",
      " 0.01960784 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.58039216 0.99215686 0.81176471 0.22745098 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.15686275 0.11764706 0.         0.         0.\n",
      " 0.         0.         0.         0.48235294 0.91764706 0.99215686\n",
      " 0.42352941 0.03137255 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.54509804 0.94117647\n",
      " 0.88627451 0.06666667 0.         0.         0.         0.\n",
      " 0.1372549  0.91372549 0.99215686 0.75294118 0.03921569 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.05490196 0.28627451 0.82745098 0.99215686 0.99215686 0.32156863\n",
      " 0.         0.         0.         0.04313725 0.42352941 0.99215686\n",
      " 0.99215686 0.68235294 0.06666667 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.27843137 0.84313725 0.99215686\n",
      " 0.99215686 0.88235294 0.48235294 0.01960784 0.         0.\n",
      " 0.         0.30588235 0.99215686 0.99215686 0.89803922 0.35294118\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.21960784 0.76078431 0.99215686 0.99215686 0.70196078 0.41176471\n",
      " 0.         0.         0.         0.         0.03921569 0.53333333\n",
      " 0.99215686 0.99215686 0.5254902  0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.02352941 0.5254902  0.99215686\n",
      " 0.99215686 0.68627451 0.36078431 0.         0.         0.\n",
      " 0.         0.         0.30196078 0.99215686 0.99215686 0.89019608\n",
      " 0.36078431 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.27843137 0.99215686 0.99215686 0.99215686 0.73333333\n",
      " 0.         0.         0.         0.         0.         0.2745098\n",
      " 0.97647059 0.99215686 0.90196078 0.16862745 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.32156863 0.95686275 0.99607843 0.99607843 0.99607843 0.83921569\n",
      " 0.52156863 0.52156863 0.52156863 0.77647059 0.99607843 0.99607843\n",
      " 0.81568627 0.01568627 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.0627451\n",
      " 0.51372549 0.92156863 0.99215686 0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.99607843 0.99215686 0.99215686 0.83921569 0.12156863\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.09019608\n",
      " 0.37647059 0.53333333 0.44313725 0.87058824 0.99215686 0.99607843\n",
      " 0.99215686 0.67843137 0.1254902  0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.65490196 0.99215686 0.99607843 0.99215686 0.53333333\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.38823529 0.83137255\n",
      " 0.99215686 0.96470588 0.28235294 0.10980392 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.39215686 0.96078431 0.99215686 0.99215686 0.81176471\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.25882353 0.64313725\n",
      " 0.99215686 0.99215686 0.83529412 0.31764706 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.09411765 0.61960784 0.99215686 0.99215686 0.99215686\n",
      " 0.56078431 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.1254902\n",
      " 0.88235294 0.99215686 0.99215686 0.40392157 0.00392157 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.01176471 0.76862745 0.83529412\n",
      " 0.3372549  0.00392157 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "[2.85075915e-01 3.70579432e-01 6.87707579e-01 7.94525570e-01\n",
      " 5.80352445e-01 8.39910628e-01 2.86022776e-02 1.64969102e-01\n",
      " 9.86530031e-02 4.81053885e-01 4.31138091e-01 3.95089216e-01\n",
      " 1.19841949e-01 4.33838406e-01 2.00414597e-01 5.89742150e-01\n",
      " 9.50095048e-01 8.20066142e-01 8.77533424e-01 5.41289173e-01\n",
      " 7.35649527e-01 1.75559839e-01 6.54232186e-01 9.37661037e-01\n",
      " 9.44065666e-02 9.74416756e-01 6.61969419e-01 6.68993364e-01\n",
      " 9.36664789e-01 9.90960126e-01 1.33934918e-01 5.39850879e-01\n",
      " 2.91317069e-01 7.55688518e-01 3.10605670e-01 9.73195581e-01\n",
      " 5.08464355e-02 9.91080898e-01 6.22460516e-01 6.54274618e-01\n",
      " 7.46748845e-03 4.95797944e-01 5.44706376e-01 6.84291136e-01\n",
      " 7.12657662e-01 9.69724750e-01 6.61908189e-01 1.16617727e-01\n",
      " 2.16803008e-01 3.50679281e-01 3.21193348e-01 6.83409416e-01\n",
      " 3.06066990e-01 7.37835563e-01 9.27112215e-01 1.34005254e-01\n",
      " 5.54583188e-03 4.33930382e-01 3.57458378e-02 9.20076632e-04\n",
      " 6.26098005e-01 2.55281158e-01 2.24772435e-01 2.78970351e-01\n",
      " 2.12747364e-01 6.21019958e-01 6.50324483e-01 9.20374530e-02\n",
      " 7.12535506e-02 1.04857037e-01 9.17207489e-03 3.42277602e-01\n",
      " 8.33915253e-01 9.95089591e-01 8.80640059e-01 1.12781491e-01\n",
      " 4.12661515e-01 2.88343574e-01 7.42528961e-01 1.44725384e-01\n",
      " 9.29099511e-01 8.31536850e-01 4.79789841e-01 5.78392627e-01\n",
      " 8.57250329e-01 6.08017439e-01 1.31856152e-01 1.80630904e-01\n",
      " 8.42217537e-01 8.11239785e-02 6.07615846e-01 2.52283483e-02\n",
      " 5.26426826e-01 8.70435017e-01 7.66052148e-01 6.10709781e-03\n",
      " 8.57563181e-02 2.65949782e-01 5.46762638e-01 4.98069670e-01\n",
      " 5.48754062e-01 7.15423183e-01 7.78177405e-01 1.57834215e-01\n",
      " 5.55461897e-01 1.66949144e-01 7.00357047e-01 2.09160208e-01\n",
      " 7.27941744e-01 8.58829406e-01 6.04221870e-02 9.20214328e-01\n",
      " 5.89165154e-02 5.20231722e-02 7.46000199e-01 7.49652170e-01\n",
      " 5.79743234e-01 4.22512989e-02 2.59945564e-01 7.12034092e-01\n",
      " 9.91519476e-01 6.39305788e-01 7.45665500e-01 6.54440908e-01\n",
      " 2.20380943e-01 7.70943466e-01 4.23770417e-01 3.44273891e-01\n",
      " 3.23166324e-01 4.08023208e-01 5.87136549e-01 5.52850960e-01\n",
      " 8.66797464e-01 4.72701579e-01 5.95997571e-01 1.96938676e-01\n",
      " 8.96512538e-01 9.71059856e-01 3.83930426e-01 2.70560593e-01\n",
      " 7.86921108e-01 6.45656751e-01 5.37072497e-01 8.09495414e-01\n",
      " 2.36962464e-01 8.44352445e-01 8.03630843e-01 8.77194847e-01\n",
      " 1.92264512e-01 3.74677787e-02 6.21150067e-01 3.56436209e-01\n",
      " 6.50601813e-01 8.72946456e-01 4.41287252e-01 1.05277006e-01\n",
      " 8.91276574e-02 8.68112826e-01 8.46257404e-01 7.36986801e-01\n",
      " 8.94801815e-01 7.32045764e-01 8.29516595e-02 1.96424388e-01\n",
      " 3.01072839e-01 6.80543031e-01 9.44075357e-01 4.51672314e-01\n",
      " 1.53598145e-01 7.37062206e-01 7.36038783e-01 3.54919913e-02\n",
      " 8.64442028e-01 8.39138020e-01 4.50354913e-01 4.93173222e-01\n",
      " 2.59339728e-01 5.45966661e-01 3.88199678e-01 3.65746751e-01\n",
      " 4.35547786e-01 4.85606734e-02 9.40480349e-02 1.91998365e-01\n",
      " 9.07230942e-01 3.06346573e-01 1.27401378e-01 5.02905955e-01\n",
      " 7.32308347e-02 7.61319311e-01 6.64695895e-01 6.59843060e-01\n",
      " 6.83086405e-01 3.68568867e-01 4.74490427e-01 3.85747337e-01\n",
      " 5.68832943e-01 3.64215009e-01 7.39259244e-01 7.62728868e-01\n",
      " 6.19238491e-01 1.74124008e-01 6.11472803e-01 1.58612424e-02\n",
      " 9.57151090e-01 1.39993251e-01 2.40083026e-01 5.22244710e-01\n",
      " 6.48879786e-01 2.91148759e-01 2.02405833e-01 8.55296660e-01\n",
      " 1.65421744e-01 4.95963476e-01 4.70107265e-01 3.62967458e-01\n",
      " 8.01124317e-02 8.76551978e-01 9.62046685e-01 9.94057415e-01\n",
      " 4.91481317e-01 1.16023172e-01 6.89932808e-01 9.62423123e-01\n",
      " 6.75224440e-01 9.72062337e-02 5.11246127e-02 3.61108288e-01\n",
      " 5.66933301e-01 9.91112472e-01 5.44125091e-01 1.22640521e-01\n",
      " 6.19030183e-01 3.20404547e-01 5.86861651e-01 4.18877252e-01\n",
      " 8.96803120e-01 1.20138963e-01 2.24143695e-01 6.95823474e-01\n",
      " 2.48927028e-01 5.27658273e-01 6.79611584e-01 3.38166075e-01\n",
      " 7.70377314e-01 7.77672135e-01 5.79176277e-01 8.83554918e-01\n",
      " 6.24333001e-02 6.13808715e-01 4.79299312e-01 5.78524756e-01\n",
      " 9.84478484e-01 2.84306728e-01 5.04852735e-01 5.78334435e-01]\n",
      "(56000, 784) (784, 256)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50.82511622 56.47719018 56.30998962 51.36140558 52.98861249 51.25895394\n",
      " 54.43589972 49.59053191 51.56211134 53.5154433  56.78772686 50.5613857\n",
      " 50.51268793 47.50880221 48.12472965 52.1038175  49.65093111 55.76807863\n",
      " 50.64593686 55.65094714 54.63003267 58.04894879 55.24644921 53.19606687\n",
      " 49.41555888 49.64466746 46.76293333 55.8590727  53.60244526 52.70079624\n",
      " 51.36547991 54.98818754 51.67600878 50.53999673 52.83183589 56.16065392\n",
      " 49.0403439  54.62416868 54.9413325  49.40138839 48.59827534 50.96955669\n",
      " 57.67070219 50.79154288 48.81816812 49.69133296 58.33173628 48.54588627\n",
      " 54.71193046 51.63485104 48.17484424 48.79654708 47.51420209 49.82294162\n",
      " 46.01373073 52.16403279 53.13412478 50.21285329 54.84832714 53.93058998\n",
      " 50.8838251  54.1307406  57.31921652 48.14177888 52.31612017 54.70238734\n",
      " 54.29366456 53.7269337  52.2894669  54.0102412  50.72759881 53.8626351\n",
      " 48.33695158 50.9576687  52.85703635 48.98615554 54.66203943 51.6177596\n",
      " 49.95632342 54.94595736 55.68512397 51.0768484  49.47752583 53.54086133\n",
      " 47.18180174 53.92372141 51.82081316 57.71494551 53.47573945 49.92353143\n",
      " 54.83326352 52.80113939 51.66569793 50.43410773 55.87294641 51.93309798\n",
      " 48.49100785 58.68662088 51.59910758 53.05957876 50.75029002 58.24620359\n",
      " 52.96483787 51.27439394 56.88629975 57.08596085 54.75007666 53.09216297\n",
      " 53.72090277 53.04453416 55.01217586 54.07517162 56.62553913 46.71820534\n",
      " 56.39053447 52.45677345 54.90076552 55.61549716 54.50275742 51.39280143\n",
      " 53.31642866 51.998066   54.17068068 53.19518774 55.92786139 53.25224484\n",
      " 50.65632488 53.41851604 51.89066634 52.970856   53.6723519  53.76410611\n",
      " 56.08888506 52.41452851 51.27353375 53.36976073 54.94513624 51.60436047\n",
      " 54.39659817 47.25286555 55.67419818 46.90232658 52.71767555 53.26285395\n",
      " 47.27935719 55.88541704 52.98058593 51.24632554 54.14425146 54.64079045\n",
      " 52.24311056 53.93201043 51.55792815 50.57059853 46.53168278 50.99762745\n",
      " 49.60945982 48.50265918 58.32157155 54.39903903 51.64624784 46.74884048\n",
      " 48.05954443 53.36358762 53.10673949 55.13244293 55.04034855 53.22643377\n",
      " 52.96906222 51.9711454  55.30247131 53.08155343 51.33024609 51.80772168\n",
      " 54.59520772 53.04863404 50.67869091 54.51587671 48.85775716 53.46070252\n",
      " 56.35600219 52.63357873 53.84889554 52.087467   47.79627789 52.55217123\n",
      " 54.346231   49.6533861  54.71446199 52.88628806 51.01122022 50.76641771\n",
      " 56.70567845 52.7188628  48.92235681 51.46137936 52.56605349 50.49938983\n",
      " 48.39089627 51.59676967 50.68717511 50.35307126 50.24698495 56.86279154\n",
      " 54.57059791 54.27238669 50.24171864 54.50153989 56.31035544 53.71366581\n",
      " 57.75874335 51.62749451 50.74443991 50.16243821 50.96324197 51.75785661\n",
      " 49.40321105 50.48908502 53.39277386 51.9840509  51.69838909 50.79597591\n",
      " 52.85251888 56.58357684 53.09024256 49.69237611 45.05321977 55.96033619\n",
      " 51.8736516  50.50975178 54.62919478 58.38953888 48.72573914 51.91843618\n",
      " 54.91977635 52.92025177 56.16403982 55.08736167 46.63415163 52.7961865\n",
      " 55.78368975 52.26987385 55.88260374 52.41253488 51.22271619 52.63649697\n",
      " 48.12748979 51.49057983 50.54125107 49.03321304 51.0370176  54.11706256\n",
      " 51.45835344 52.30045277 48.79819609 51.40378467]\n",
      "[50.82511622 56.47719018 56.30998962 51.36140558 52.98861249 51.25895394\n",
      " 54.43589972 49.59053191 51.56211134 53.5154433  56.78772686 50.5613857\n",
      " 50.51268793 47.50880221 48.12472965 52.1038175  49.65093111 55.76807863\n",
      " 50.64593686 55.65094714 54.63003267 58.04894879 55.24644921 53.19606687\n",
      " 49.41555888 49.64466746 46.76293333 55.8590727  53.60244526 52.70079624\n",
      " 51.36547991 54.98818754 51.67600878 50.53999673 52.83183589 56.16065392\n",
      " 49.0403439  54.62416868 54.9413325  49.40138839 48.59827534 50.96955669\n",
      " 57.67070219 50.79154288 48.81816812 49.69133296 58.33173628 48.54588627\n",
      " 54.71193046 51.63485104 48.17484424 48.79654708 47.51420209 49.82294162\n",
      " 46.01373073 52.16403279 53.13412478 50.21285329 54.84832714 53.93058998\n",
      " 50.8838251  54.1307406  57.31921652 48.14177888 52.31612017 54.70238734\n",
      " 54.29366456 53.7269337  52.2894669  54.0102412  50.72759881 53.8626351\n",
      " 48.33695158 50.9576687  52.85703635 48.98615554 54.66203943 51.6177596\n",
      " 49.95632342 54.94595736 55.68512397 51.0768484  49.47752583 53.54086133\n",
      " 47.18180174 53.92372141 51.82081316 57.71494551 53.47573945 49.92353143\n",
      " 54.83326352 52.80113939 51.66569793 50.43410773 55.87294641 51.93309798\n",
      " 48.49100785 58.68662088 51.59910758 53.05957876 50.75029002 58.24620359\n",
      " 52.96483787 51.27439394 56.88629975 57.08596085 54.75007666 53.09216297\n",
      " 53.72090277 53.04453416 55.01217586 54.07517162 56.62553913 46.71820534\n",
      " 56.39053447 52.45677345 54.90076552 55.61549716 54.50275742 51.39280143\n",
      " 53.31642866 51.998066   54.17068068 53.19518774 55.92786139 53.25224484\n",
      " 50.65632488 53.41851604 51.89066634 52.970856   53.6723519  53.76410611\n",
      " 56.08888506 52.41452851 51.27353375 53.36976073 54.94513624 51.60436047\n",
      " 54.39659817 47.25286555 55.67419818 46.90232658 52.71767555 53.26285395\n",
      " 47.27935719 55.88541704 52.98058593 51.24632554 54.14425146 54.64079045\n",
      " 52.24311056 53.93201043 51.55792815 50.57059853 46.53168278 50.99762745\n",
      " 49.60945982 48.50265918 58.32157155 54.39903903 51.64624784 46.74884048\n",
      " 48.05954443 53.36358762 53.10673949 55.13244293 55.04034855 53.22643377\n",
      " 52.96906222 51.9711454  55.30247131 53.08155343 51.33024609 51.80772168\n",
      " 54.59520772 53.04863404 50.67869091 54.51587671 48.85775716 53.46070252\n",
      " 56.35600219 52.63357873 53.84889554 52.087467   47.79627789 52.55217123\n",
      " 54.346231   49.6533861  54.71446199 52.88628806 51.01122022 50.76641771\n",
      " 56.70567845 52.7188628  48.92235681 51.46137936 52.56605349 50.49938983\n",
      " 48.39089627 51.59676967 50.68717511 50.35307126 50.24698495 56.86279154\n",
      " 54.57059791 54.27238669 50.24171864 54.50153989 56.31035544 53.71366581\n",
      " 57.75874335 51.62749451 50.74443991 50.16243821 50.96324197 51.75785661\n",
      " 49.40321105 50.48908502 53.39277386 51.9840509  51.69838909 50.79597591\n",
      " 52.85251888 56.58357684 53.09024256 49.69237611 45.05321977 55.96033619\n",
      " 51.8736516  50.50975178 54.62919478 58.38953888 48.72573914 51.91843618\n",
      " 54.91977635 52.92025177 56.16403982 55.08736167 46.63415163 52.7961865\n",
      " 55.78368975 52.26987385 55.88260374 52.41253488 51.22271619 52.63649697\n",
      " 48.12748979 51.49057983 50.54125107 49.03321304 51.0370176  54.11706256\n",
      " 51.45835344 52.30045277 48.79819609 51.40378467]\n",
      "[[6525.16177845 6810.19474159 6744.66566418 ... 6519.88970929\n",
      "  6980.23280906 6440.43490556]\n",
      " [3929.55519365 4115.06414272 4081.46309709 ... 3929.07858745\n",
      "  4230.62169734 3890.84870898]\n",
      " [4331.12156485 4536.67027252 4494.73696585 ... 4333.44260819\n",
      "  4651.04543921 4293.74223383]\n",
      " ...\n",
      " [6225.16795296 6496.00501104 6453.33087021 ... 6234.26431959\n",
      "  6696.44806677 6160.40015665]\n",
      " [3765.57247679 3948.43047642 3905.24053686 ... 3771.8765477\n",
      "  4068.72254198 3736.69707452]\n",
      " [4953.30849271 5170.54151606 5135.30977518 ... 4940.71189912\n",
      "  5318.93526403 4888.78056381]]\n",
      "[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (560000,560000) (56000,56000) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-ea0fd7565ff2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mepochs_arr\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabsolute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-75-7e38d773cc83>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs, learning_rate)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m#error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mderivative_predicted_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msoftmax_derivative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0merror_hidden_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mderivative_predicted_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_layer_weight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-74-9c26342669ea>\u001b[0m in \u001b[0;36msoftmax_derivative\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msoftmax_derivative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mjac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiagflat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (560000,560000) (56000,56000) "
     ]
    }
   ],
   "source": [
    "epochs_arr , loss = train(learning_rate=(0.1))\n",
    "loss = np.absolute(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [20,10]\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.plot(epochs_arr, loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
