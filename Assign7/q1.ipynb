{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11914"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "file = open('./all_sentiment_shuffled.txt')\n",
    "\n",
    "for line in file:\n",
    "    line = line.split(' ')\n",
    "    row = []\n",
    "    row.append(line[0])\n",
    "    row.append(line[1])\n",
    "    row.append(line[2])\n",
    "    row.append(' '.join(line[3:]).translate(str.maketrans('', '', string.punctuation)).rstrip())\n",
    "    data.append(row)\n",
    "    \n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54090\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "vocabulary = set([])\n",
    "classes = set([])\n",
    "for d in data:\n",
    "    classes.add(d[1])\n",
    "    line = d[3]\n",
    "    for w in line.split(' '):\n",
    "        if len(w) == 0:\n",
    "            continue\n",
    "        vocabulary.add(w)\n",
    "        \n",
    "print(len(vocabulary))    \n",
    "print(len(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "def getSubset(train_data, c):\n",
    "    subset = []\n",
    "    for d in train_data:\n",
    "        if (d[1] == c):\n",
    "            subset.append(d)\n",
    "    return subset\n",
    "\n",
    "print(len(getSubset(data[:5], 'neg')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def makeMegaDoc(subset_docs):\n",
    "    mega_doc = {}\n",
    "    for d in subset_docs:\n",
    "        line = d[3].split(' ')\n",
    "        for w in line:\n",
    "            if w == '':\n",
    "                continue\n",
    "            if w in mega_doc:\n",
    "                mega_doc[w] += 1\n",
    "            else:\n",
    "                mega_doc[w] = 1\n",
    "                \n",
    "    return mega_doc\n",
    "\n",
    "# print(makeMegaDoc(data[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "def getFrequency(mega_doc, w):\n",
    "    if w in mega_doc:\n",
    "        return mega_doc[w]\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "print(getFrequency(makeMegaDoc(data[:5]), 'and'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_data, classes, vocabulary, smoothing_factor=1):\n",
    "    probab_class = {}\n",
    "    probab_conditional_word = {}\n",
    "    for c in classes:\n",
    "        subset_docs = getSubset(train_data, c)\n",
    "        probab_class[c] = len(subset_docs) / len(train_data)\n",
    "        \n",
    "        mega_doc = makeMegaDoc(subset_docs)\n",
    "        for w in vocabulary:\n",
    "            nk = getFrequency(mega_doc, w)\n",
    "            probab_conditional_word[w + '|' + c] = (nk + smoothing_factor) / (len(mega_doc) + smoothing_factor*len(vocabulary))\n",
    "    \n",
    "    return probab_class, probab_conditional_word\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(test_data, classes, probab_class, probab_conditional_word):\n",
    "    \n",
    "    prediction = []\n",
    "    \n",
    "    for d in test_data:\n",
    "        max_prob = 0\n",
    "        max_class = ''\n",
    "        \n",
    "        for c in classes:\n",
    "            curprob = probab_class[c]\n",
    "            for w in d[3].split(' '):\n",
    "                if w == '':\n",
    "                    continue\n",
    "                    \n",
    "                curprob = curprob * (probab_conditional_word[w + '|' + c])\n",
    "            \n",
    "            if curprob >= max_prob:\n",
    "                max_prob = curprob\n",
    "                max_class = c\n",
    "\n",
    "        prediction.append(max_class)\n",
    "        \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getActualLabels(act_data):\n",
    "    act_labels = []\n",
    "    for d in act_data:\n",
    "        act_labels.append(d[1])\n",
    "    return act_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataInIndex(data, index):\n",
    "    l = []\n",
    "    for i in range(len(data)):\n",
    "        if i in index:\n",
    "            l.append(data[i])\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Score = 0.891110675437585\n",
      "Recall Score = 0.4624696542747241\n",
      "F Score = 0.6084148331386691\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(5, True, 1)\n",
    "precision = []\n",
    "recall = []\n",
    "f_score = []\n",
    "\n",
    "for trainInd,testInd in kfold.split(data):\n",
    "    train_data = getDataInIndex(data, trainInd)\n",
    "    test_data = getDataInIndex(data, testInd)\n",
    "    \n",
    "    probab_class, probab_conditional_word = train(train_data, classes, vocabulary)\n",
    "    prediction = classify(test_data, classes, probab_class, probab_conditional_word)\n",
    "        \n",
    "    actual = getActualLabels(test_data)\n",
    "    predicted = prediction\n",
    "    \n",
    "#     print(classification_report(actual, predicted))\n",
    "    precision.append(precision_score(actual, predicted, pos_label=\"pos\"))\n",
    "    recall.append(recall_score(actual, predicted, pos_label=\"pos\"))\n",
    "    f_score.append(f1_score(actual, predicted, pos_label=\"pos\"))\n",
    "\n",
    "print(\"Precision Score = \" + str(mean(precision)))\n",
    "print(\"Recall Score = \" + str(mean(recall)))\n",
    "print(\"F Score = \" + str(mean(f_score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
